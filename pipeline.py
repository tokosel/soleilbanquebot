import os
import json
from ingestion.document_loader import DocumentLoader
from ingestion.text_processor import TextProcessor
from ingestion.chunker import Chunker
from ingestion.indexer import Indexer

RAW_DOCS_DIR = "data/documents"
PROCESSED_DIR = "data/processed/chunks"
VECTOR_DB_PATH = "data/vector_store/chroma_db"
TRACK_FILE = "data/processed/ingested_files.json"

def load_previous_ingestions():
    if os.path.exists(TRACK_FILE):
        with open(TRACK_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_ingested_files(filenames):
    with open(TRACK_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(filenames), f, indent=2)

def run_ingestion():
    print("\nğŸš€ [0] DÃ‰MARRAGE DU PIPELINE D'INGESTION...\n")

    # ğŸ” Ã‰tape 1 : VÃ©rifier les nouveaux fichiers PDF
    all_files = set(f for f in os.listdir(RAW_DOCS_DIR) if f.endswith(".pdf"))
    already_ingested = load_previous_ingestions()
    new_files = all_files - already_ingested

    if not new_files:
        print("âœ… Aucun nouveau document Ã  ingÃ©rer. Tous les fichiers ont dÃ©jÃ  Ã©tÃ© traitÃ©s.\n")
        return

    print(f"ğŸ“‚ [1] {len(new_files)} nouveau(x) fichier(s) dÃ©tectÃ©(s) :")
    for f in new_files:
        print(f"   â””â”€ {f}")

    # ğŸ“¥ Ã‰tape 2 : Charger les nouveaux fichiers PDF
    loader = DocumentLoader(RAW_DOCS_DIR)
    all_loaded_docs = loader.load_pdfs()
    documents = {name: text for name, text in all_loaded_docs.items() if name in new_files}
    print(f"\nğŸ“¥ [2] {len(documents)} document(s) PDF chargÃ©(s) avec succÃ¨s.")

    # ğŸ§¹ Ã‰tape 3 : Nettoyage du texte
    processor = TextProcessor()
    cleaned_documents = {}
    for name, text in documents.items():
        cleaned_documents[name] = processor.clean_text(text)
        print(f"   âœ”ï¸ Texte nettoyÃ© pour : {name}")

    # âœ‚ï¸ Ã‰tape 4 : DÃ©coupage en chunks
    chunker = Chunker()
    chunked_documents = {}
    for name, text in cleaned_documents.items():
        chunks = chunker.chunk_text(text)
        chunked_documents[name] = chunks
        print(f"   ğŸ“ {len(chunks)} chunk(s) gÃ©nÃ©rÃ©(s) pour : {name}")

    # ğŸ’¾ Ã‰tape 5 : Sauvegarde locale des chunks
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    for name, chunks in chunked_documents.items():
        path = os.path.join(PROCESSED_DIR, f"{name}.txt")
        with open(path, "w", encoding="utf-8") as f:
            for chunk in chunks:
                f.write(chunk + "\n\n")
        print(f"   ğŸ’¾ Chunks sauvegardÃ©s dans : {path}")

    # ğŸ§  Ã‰tape 6 : Indexation dans ChromaDB
    indexer = Indexer(VECTOR_DB_PATH)
    for name, chunks in chunked_documents.items():
        indexer.index_chunks(chunks, name)
        print(f"   ğŸ§  {len(chunks)} chunk(s) indexÃ©(s) pour : {name}")

    # ğŸ“ Ã‰tape 7 : Mise Ã  jour de lâ€™historique des fichiers traitÃ©s
    save_ingested_files(all_files)
    print("\nğŸ“œ Historique des fichiers mis Ã  jour.")

    # âœ… Fin du pipeline
    print("\nğŸ‰ INGESTION TERMINÃ‰E AVEC SUCCÃˆS ! Tous les nouveaux fichiers ont Ã©tÃ© traitÃ©s et indexÃ©s.\n")

if __name__ == "__main__":
    run_ingestion()
